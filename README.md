# Урок 6: Трансформер и машинный перевод

В этом уроке мы реализуем с нуля архитектуру Transformer и применим её к задаче машинного перевода. Мы также научимся оптимизировать потребление памяти и скорость вычислений за счет понижения точности (mixed precision training).

### Токенизатор

Для токенизации используется библиотека `tokenizers` с предобученным токенизатором от Mistral-7b.

### Оптимизация производительности

В уроке используется mixed precision training для оптимизации:
- **Памяти**: использование float16 вместо float32 снижает потребление памяти примерно в 2 раза
- **Скорости**: современные GPU быстрее работают с float16
- **Автоматическое управление**: `torch.amp.autocast` автоматически переключает точность

```python
from torch.amp import autocast

with autocast(device_type='cuda', dtype=torch.float16):
    logits = model(src_ids, tgt_ids)
```